{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38016c-7f0f-4c45-8c06-588afdcb8fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26cc80-5a84-48ab-ac33-715a101f61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f97320-5fd2-436a-8ef1-6d85957407e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_user = 'sisaman'\n",
    "wandb_project = 'GAP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d735c43-3800-45a8-bec1-a57573459325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunFactory:\n",
    "    def __init__(self, entity, project, check_existing=True):\n",
    "        self.project = project\n",
    "        self.check_existing = check_existing\n",
    "\n",
    "        if check_existing:\n",
    "            api = wandb.Api()\n",
    "            runs = api.runs(f\"{entity}/{project}\", per_page=2000)\n",
    "            config_list = []\n",
    "            for run in runs:\n",
    "                config_list.append({k: v for k,v in run.config.items() if not k.startswith('_')})\n",
    "\n",
    "            self.runs_df = pd.DataFrame.from_dict(config_list)\n",
    "            self.runs_df['epsilon'] = self.runs_df['epsilon'].astype(float)\n",
    "        \n",
    "    \n",
    "    def build(self, subcommand, **params):      \n",
    "        for key, value in params.items():\n",
    "            if not (isinstance(value, list) or isinstance(value, tuple)):\n",
    "                params[key] = (value,)\n",
    "        \n",
    "        cmd_list = []\n",
    "        configs = self.product_dict(params)\n",
    "\n",
    "        for config in configs:\n",
    "            if not self.check_existing or len(self.find_runs(config)) == 0:\n",
    "                config['project'] = self.project\n",
    "                options = ' '.join([f' --{param} {value} ' for param, value in config.items()])\n",
    "                command = f'python train.py {subcommand} {options}'\n",
    "                command = ' '.join(command.split())\n",
    "                cmd_list.append(command)\n",
    "\n",
    "        return cmd_list\n",
    "\n",
    "    def find_runs(self, config):\n",
    "        test_df = self.runs_df.loc[np.all([self.runs_df[k] == v for k, v in config.items()], axis=0), :]\n",
    "        return test_df\n",
    "\n",
    "    @staticmethod\n",
    "    def product_dict(params):\n",
    "        keys = params.keys()\n",
    "        vals = params.values()\n",
    "        for instance in product(*vals):\n",
    "            yield dict(zip(keys, instance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd51b9-3ff7-42ff-88f8-9fbf7bc5f675",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e925c-8c31-4491-86fa-40387b9b4489",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_factory = RunFactory(entity=wandb_user, project=wandb_project, check_existing=True)\n",
    "\n",
    "# DEFAULT PARAMS\n",
    "dataset=['facebook', 'reddit', 'amazon']\n",
    "epsilon={\n",
    "    'edge': {\n",
    "        'standard': list(range(1,10,2)),\n",
    "        'extended': [0.2, 0.4, 0.6, 0.8] + list(range(1,10,2)),\n",
    "    },\n",
    "    'node': {\n",
    "        'standard': list(range(5,30,5)),\n",
    "        'extended': [1, 2, 3, 4] + list(range(5,30,5)),\n",
    "    }\n",
    "}\n",
    "hops=[1,2,3,4,5]\n",
    "max_degree = {\n",
    "    'edge': -1,\n",
    "    'node': {\n",
    "        'facebook': {\n",
    "            'standard': [10,20,50,100,200],\n",
    "            'extended': [10,20,50,100,200],\n",
    "        },\n",
    "        'reddit': {\n",
    "            'standard': [50,100,200,300,400],\n",
    "            'extended': [50,100,200,300,400],\n",
    "        },\n",
    "        'amazon': {\n",
    "            'standard': [10,20,50,100,200],\n",
    "            'extended': [10,20,50,100,200],\n",
    "        },\n",
    "    }\n",
    "}\n",
    "hidden_dim=[16]\n",
    "encoder_layers=2\n",
    "pre_layers=1\n",
    "post_layers=1\n",
    "combine='cat'\n",
    "activation='selu'\n",
    "dropout=0\n",
    "batch_norm=True\n",
    "optimizer='adam'\n",
    "learning_rate=0.01,\n",
    "weight_decay=0,\n",
    "pre_epochs = {\n",
    "    'edge': 100,\n",
    "    'node': 10,\n",
    "}\n",
    "epochs = {\n",
    "    'edge': 100,\n",
    "    'node': 10,\n",
    "}\n",
    "batch_size = {\n",
    "    'edge': -1,\n",
    "    'node': {\n",
    "        'facebook': 256,\n",
    "        'reddit':   2048,\n",
    "        'amazon':   4096,\n",
    "    }\n",
    "}\n",
    "max_grad_norm=1\n",
    "repeats=10\n",
    "logger='wandb'\n",
    "\n",
    "cmd = []\n",
    "\n",
    "cmd += run_factory.build(\n",
    "    subcommand='gap',\n",
    "    name='GAP-INF',\n",
    "    dataset=dataset,\n",
    "    dp_level='edge',\n",
    "    epsilon=np.inf,\n",
    "    hops=hops,\n",
    "    max_degree=max_degree['edge'],\n",
    "    hidden_dim=hidden_dim,\n",
    "    encoder_layers=encoder_layers,\n",
    "    pre_layers=pre_layers,\n",
    "    post_layers=post_layers,\n",
    "    combine=combine,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    batch_norm=batch_norm,\n",
    "    optimizer=optimizer,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    pre_epochs=pre_epochs['edge'],\n",
    "    epochs=epochs['edge'],\n",
    "    batch_size=batch_size['edge'],\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    repeats=repeats,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "cmd += run_factory.build(\n",
    "    subcommand='gap',\n",
    "    name='GAP-EDP',\n",
    "    dataset=dataset,\n",
    "    dp_level='edge',\n",
    "    epsilon=epsilon['edge']['extended'],\n",
    "    hops=hops,\n",
    "    max_degree=max_degree['edge'],\n",
    "    hidden_dim=hidden_dim,\n",
    "    encoder_layers=encoder_layers,\n",
    "    pre_layers=pre_layers,\n",
    "    post_layers=post_layers,\n",
    "    combine=combine,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    batch_norm=batch_norm,\n",
    "    optimizer=optimizer,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    pre_epochs=pre_epochs['edge'],\n",
    "    epochs=epochs['edge'],\n",
    "    batch_size=batch_size['edge'],\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    repeats=repeats,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# GAP-EDP W/O EM\n",
    "cmd += run_factory.build(\n",
    "    subcommand='gap',\n",
    "    name='GAP-EDP',\n",
    "    dataset=dataset,\n",
    "    dp_level='edge',\n",
    "    epsilon=epsilon['edge']['standard'],\n",
    "    hops=hops,\n",
    "    max_degree=max_degree['edge'],\n",
    "    hidden_dim=hidden_dim,\n",
    "    encoder_layers=0,\n",
    "    pre_layers=pre_layers,\n",
    "    post_layers=post_layers,\n",
    "    combine=combine,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    batch_norm=batch_norm,\n",
    "    optimizer=optimizer,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    pre_epochs=0,\n",
    "    epochs=epochs['edge'],\n",
    "    batch_size=batch_size['edge'],\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    repeats=repeats,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "\n",
    "cmd += run_factory.build(\n",
    "    subcommand='gap',\n",
    "    name='MLP',\n",
    "    dataset=dataset,\n",
    "    dp_level='edge',\n",
    "    epsilon=0,\n",
    "    hops=0,\n",
    "    max_degree=max_degree['edge'],\n",
    "    hidden_dim=hidden_dim,\n",
    "    encoder_layers=0,\n",
    "    pre_layers=encoder_layers,\n",
    "    post_layers=post_layers,\n",
    "    combine=combine,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    batch_norm=batch_norm,\n",
    "    optimizer=optimizer,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    pre_epochs=0,\n",
    "    epochs=epochs['edge'],\n",
    "    batch_size=batch_size['edge'],\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    repeats=repeats,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "cmd += run_factory.build(\n",
    "    subcommand='sage',\n",
    "    name='SAGE-EDP',\n",
    "    dataset=dataset,\n",
    "    dp_level='edge',\n",
    "    epsilon=epsilon['edge']['extended'],\n",
    "    max_degree=max_degree['edge'],\n",
    "    hidden_dim=hidden_dim,\n",
    "    encoder_layers=encoder_layers,\n",
    "    mp_layers=hops,\n",
    "    post_layers=post_layers,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    batch_norm=batch_norm,\n",
    "    optimizer=optimizer,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    epochs=epochs['edge'],\n",
    "    batch_size=batch_size['edge'],\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    repeats=repeats,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "cmd += run_factory.build(\n",
    "    subcommand='sage',\n",
    "    name='SAGE-INF',\n",
    "    dataset=dataset,\n",
    "    dp_level='edge',\n",
    "    epsilon=np.inf,\n",
    "    max_degree=max_degree['edge'],\n",
    "    hidden_dim=hidden_dim,\n",
    "    encoder_layers=encoder_layers,\n",
    "    mp_layers=hops,\n",
    "    post_layers=post_layers,\n",
    "    activation=activation,\n",
    "    dropout=dropout,\n",
    "    batch_norm=batch_norm,\n",
    "    optimizer=optimizer,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    epochs=epochs['edge'],\n",
    "    batch_size=batch_size['edge'],\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    repeats=repeats,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "for dataset_name in dataset:\n",
    "    cmd += run_factory.build(\n",
    "        subcommand='gap',\n",
    "        name='GAP-NDP',\n",
    "        dataset=dataset_name,\n",
    "        dp_level='node',\n",
    "        epsilon=epsilon['node']['extended'],\n",
    "        hops=hops,\n",
    "        max_degree=max_degree['node'][dataset_name]['extended'],\n",
    "        hidden_dim=hidden_dim,\n",
    "        encoder_layers=encoder_layers,\n",
    "        pre_layers=pre_layers,\n",
    "        post_layers=post_layers,\n",
    "        combine=combine,\n",
    "        activation=activation,\n",
    "        dropout=dropout,\n",
    "        batch_norm=False,\n",
    "        optimizer=optimizer,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        pre_epochs=pre_epochs['node'],\n",
    "        epochs=epochs['node'],\n",
    "        batch_size=batch_size['node'][dataset_name],\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        repeats=repeats,\n",
    "        logger=logger\n",
    "    )\n",
    "    \n",
    "    # GAP-NDP W/O EM\n",
    "    cmd += run_factory.build(\n",
    "        subcommand='gap',\n",
    "        name='GAP-NDP',\n",
    "        dataset=dataset_name,\n",
    "        dp_level='node',\n",
    "        epsilon=epsilon['node']['standard'],\n",
    "        hops=hops,\n",
    "        max_degree=max_degree['node'][dataset_name]['standard'],\n",
    "        hidden_dim=hidden_dim,\n",
    "        encoder_layers=0,\n",
    "        pre_layers=pre_layers,\n",
    "        post_layers=post_layers,\n",
    "        combine=combine,\n",
    "        activation=activation,\n",
    "        dropout=dropout,\n",
    "        batch_norm=False,\n",
    "        optimizer=optimizer,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        pre_epochs=0,\n",
    "        epochs=epochs['node'],\n",
    "        batch_size=batch_size['node'][dataset_name],\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        repeats=repeats,\n",
    "        logger=logger\n",
    "    )\n",
    "    \n",
    "    cmd += run_factory.build(\n",
    "        subcommand='sage',\n",
    "        name='SAGE-NDP',\n",
    "        dataset=dataset_name,\n",
    "        dp_level='node',\n",
    "        epsilon=epsilon['node']['extended'],\n",
    "        max_degree=max_degree['node'][dataset_name]['standard'],\n",
    "        hidden_dim=hidden_dim,\n",
    "        encoder_layers=encoder_layers,\n",
    "        mp_layers=1,\n",
    "        post_layers=post_layers,\n",
    "        activation=activation,\n",
    "        dropout=dropout,\n",
    "        batch_norm=False,\n",
    "        optimizer=optimizer,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        epochs=epochs['node'],\n",
    "        batch_size=batch_size['node'][dataset_name],\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        repeats=repeats,\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    cmd += run_factory.build(\n",
    "        subcommand='gap',\n",
    "        name='MLP-DP',\n",
    "        dataset=dataset_name,\n",
    "        dp_level='node',\n",
    "        epsilon=epsilon['node']['extended'],\n",
    "        hops=0,\n",
    "        max_degree=-1,\n",
    "        hidden_dim=hidden_dim,\n",
    "        encoder_layers=0,\n",
    "        pre_layers=encoder_layers,\n",
    "        post_layers=post_layers,\n",
    "        combine=combine,\n",
    "        activation=activation,\n",
    "        dropout=dropout,\n",
    "        batch_norm=False,\n",
    "        optimizer=optimizer,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        pre_epochs=0,\n",
    "        epochs=epochs['node'],\n",
    "        batch_size=batch_size['node'][dataset_name],\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        repeats=repeats,\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "\n",
    "# shuffle(cmd)\n",
    "\n",
    "filename = 'jobs/experiments.sh'\n",
    "os.makedirs('jobs', exist_ok=True)\n",
    "with open(filename, 'w') as file:\n",
    "    for item in cmd:\n",
    "        print(item, file=file)\n",
    "\n",
    "print('new:        ', len(cmd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986afad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1690ba48bac90b79f9fd48f79bebd053e96f578825c64a887e5d9c95b0e7c0e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
